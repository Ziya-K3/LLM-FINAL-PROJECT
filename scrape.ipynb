{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd20678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for Glioma...\n",
      "Fetching data for Meningioma...\n",
      "Fetching data for Pituitary Tumor...\n",
      "Fetching data for No Tumor...\n",
      "Data scraping complete and saved to 'brain_tumor_mri_data.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define search terms for each tumor class\n",
    "search_terms = {\n",
    "    \"Glioma\": \"Glioma MRI interpretation\",\n",
    "    \"Meningioma\": \"Meningioma MRI interpretation\",\n",
    "    \"Pituitary Tumor\": \"Pituitary tumor MRI interpretation\",\n",
    "    \"No Tumor\": \"Normal brain MRI interpretation\"\n",
    "}\n",
    "\n",
    "# Define the base URL for PubMed Central's E-utilities\n",
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "# Function to fetch PubMed IDs (PMIDs) based on search terms\n",
    "def fetch_pmids(search_term):\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": search_term,\n",
    "        \"retmax\": \"100\",  # Number of results to fetch\n",
    "        \"retmode\": \"xml\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    root = ET.fromstring(response.content)\n",
    "    return [id_elem.text for id_elem in root.findall(\".//Id\")]\n",
    "\n",
    "# Function to fetch article details using PMIDs\n",
    "def fetch_article_details(pmids):\n",
    "    ids = \",\".join(pmids)\n",
    "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": ids,\n",
    "        \"retmode\": \"xml\"\n",
    "    }\n",
    "    response = requests.get(fetch_url, params=params)\n",
    "    root = ET.fromstring(response.content)\n",
    "    articles = []\n",
    "    for docsum in root.findall(\".//DocSum\"):\n",
    "        article = {}\n",
    "        for item in docsum.findall(\"Item\"):\n",
    "            name = item.get(\"Name\")\n",
    "            if name == \"Title\":\n",
    "                article[\"title\"] = item.text\n",
    "            elif name == \"Source\":\n",
    "                article[\"journal\"] = item.text\n",
    "            elif name == \"PubDate\":\n",
    "                article[\"publication_date\"] = item.text\n",
    "            elif name == \"AuthorList\":\n",
    "                authors = [author.text for author in item.findall(\"Author\")]\n",
    "                article[\"authors\"] = authors\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def scrape_and_save_data():\n",
    "    all_data = {}\n",
    "    for tumor_class, search_term in search_terms.items():\n",
    "        print(f\"Fetching data for {tumor_class}...\")\n",
    "        pmids = fetch_pmids(search_term)\n",
    "        articles = fetch_article_details(pmids)\n",
    "        all_data[tumor_class] = articles\n",
    "\n",
    "    # Save the collected data to a JSON file\n",
    "    with open(\"brain_tumor_mri_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Data scraping complete and saved to 'brain_tumor_mri_data.json'.\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_and_save_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dc6131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Glioma\n",
      "Processing class: Meningioma\n",
      "Processing class: Pituitary Tumor\n",
      "Error fetching abstract for: Ectopic Thyrotropin-Secreting Tumor in the Nasopharynx Causing Central Hyperthyroidism.\n",
      "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "Processing class: No Tumor\n",
      "Error fetching abstract for: Characterising ongoing brain aging and baseline effects from cross-sectional data.\n",
      "500 Server Error: Internal Server Error for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=Characterising+ongoing+brain+aging+and+baseline+effects+from+cross-sectional+data.&retmode=json\n",
      "Enriched JSON saved to 'articles_with_abstracts.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Load your existing JSON\n",
    "with open(\"brain_tumor_mri_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def fetch_abstract_by_title(title):\n",
    "    # Step 1: Use ESearch to get PMID\n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    search_params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": title,\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "    r = requests.get(search_url, params=search_params)\n",
    "    r.raise_for_status()\n",
    "    id_list = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "    if not id_list:\n",
    "        return \"\"  # No match found\n",
    "    pmid = id_list[0]\n",
    "\n",
    "    # Step 2: Use EFetch to get abstract\n",
    "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    fetch_params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": pmid,\n",
    "        \"retmode\": \"xml\"\n",
    "    }\n",
    "    r = requests.get(fetch_url, params=fetch_params)\n",
    "    r.raise_for_status()\n",
    "    root = ET.fromstring(r.content)\n",
    "    abstract_elem = root.find(\".//Abstract\")\n",
    "    if abstract_elem is None:\n",
    "        return \"\"\n",
    "    abstract_text = \" \".join([p.text for p in abstract_elem.findall(\"AbstractText\") if p.text])\n",
    "    return abstract_text\n",
    "\n",
    "# Add abstracts to each article\n",
    "for class_name, articles in data.items():\n",
    "    print(f\"Processing class: {class_name}\")\n",
    "    for article in articles:\n",
    "        title = article.get(\"title\")\n",
    "        if title:\n",
    "            try:\n",
    "                article[\"abstract\"] = fetch_abstract_by_title(title)\n",
    "                time.sleep(0.3)  # avoid hitting PubMed too fast\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching abstract for: {title}\\n{e}\")\n",
    "                article[\"abstract\"] = \"\"\n",
    "\n",
    "# Save enriched JSON\n",
    "with open(\"articles_with_abstracts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Enriched JSON saved to 'articles_with_abstracts.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c277aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summaries written to tumor_class_summaries.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Input: your enriched JSON\n",
    "INPUT_FILE = \"articles_with_abstracts.json\"\n",
    "OUTPUT_FILE = \"tumor_class_summaries.json\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean PubMed abstracts: remove citations, normalize spaces.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # remove numeric citations like [1], [12]\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "    # remove inline references with years e.g. (Smith et al., 2021)\n",
    "    text = re.sub(r\"\\([^\\)]*\\d{4}[^\\)]*\\)\", \"\", text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def build_class_summaries(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    summaries = {}\n",
    "\n",
    "    for class_name, articles in data.items():\n",
    "        all_cleaned = []\n",
    "        for article in articles:\n",
    "            abs_text = article.get(\"abstract\", \"\")\n",
    "            cleaned = clean_text(abs_text)\n",
    "            if cleaned:\n",
    "                all_cleaned.append(cleaned)\n",
    "\n",
    "        # merge all abstracts into one long string per class\n",
    "        merged_text = \" \".join(all_cleaned)\n",
    "\n",
    "        summaries[class_name] = {\n",
    "            \"num_articles\": len(all_cleaned),\n",
    "            \"summary_text\": merged_text\n",
    "        }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summaries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Summaries written to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_class_summaries(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
